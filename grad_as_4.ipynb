{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour, when, to_timestamp\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UserClicksHashing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file containing user click timestamps\n",
    "clicks_df = spark.read.csv(\"gs://ibd_bucket-as/input.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Parse the timestamp column to a timestamp type\n",
    "clicks_df = clicks_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"HH:mm\"))\n",
    "\n",
    "def hash_timestamp(hour_value):\n",
    "    return when((hour_value >= 0) & (hour_value < 6), \"0-6\") \\\n",
    "           .when((hour_value >= 6) & (hour_value < 12), \"6-12\") \\\n",
    "           .when((hour_value >= 12) & (hour_value < 18), \"12-18\") \\\n",
    "           .otherwise(\"18-24\")\n",
    "\n",
    "# Apply the hash function to the timestamps and count the clicks in each bucket\n",
    "clicks_count_df = clicks_df.select(hour(col(\"timestamp\")).alias(\"hour\")) \\\n",
    "    .groupBy(hash_timestamp(col(\"hour\")).alias(\"time_interval\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"time_interval\")\n",
    "\n",
    "# Show the resulting counts\n",
    "clicks_count_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
